Assume your dataframe is named df and the columns are « col1 » and « col2 ».
You can get the value of « col1 » where « col2 » is maximum like this:

df.loc[df['col2'].idxmax(), 'col1']
________________________________________________________________________
Plot the graph of CPR as a function of moneyness. D
As moneyness is a continuous variable we group by quantiles. We plot the CPR of each group as a function of the mean moneyness in the group.

df5['moneyness_q'] = pd.qcut(df5['moneyness'], q=10)
df5['moneyness_q'] = df5['moneyness_q'].apply(lambda interval: (interval.left + interval.right) / 2)

or 

df['Yr_cut'] = pd.cut(df.Year, bins=3, labels=['old', 'medium', 'new'])
________________________________________________________________________
to display the whole columns not truncated
pd.set_option('display.max_colwidth', None)
________________________________________________________________________
Use regex to extract the first number (with possible commas)
df['hh_income_lb'] = df['hh_income'].str.extract(r'([\d,]+)')[0]          # grab first number
df['hh_income_lb'] = df['hh_income_lb'].str.replace(',', '', regex=True)  # remove commas
df['hh_income_lb'] = df['hh_income_lb'].astype(float)   
________________________________________________________________________
other

s = "(1.086, 1.853]"
first_num = float(s.split(',')[0][1:])  # Remove '(' and convert to float
print(first_num)  # Output: 1.086
________________________________________________________________________
If you want to extract the first float from any string, regardless of how the string looks (it may have brackets, spaces, other text, etc.), regex is the most robust way.
import re

s = "(1.086, 1.853]"
match = re.search(r'[-+]?\d*\.\d+|\d+', s)
if match:
    first_float = float(match.group())
    print(first_float)
else:
    print("No float found.")
________________________________________________________________________
plot nan heatmap
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt


plt.figure(figsize=(10, 6))
sns.heatmap(df.isnull(), cbar=False, cmap='gray')  # White is non-NaN, black is NaN
plt.show()
________________________________________________________________________
for duplicated values including first occurrence
all_dupes = early_df[early_df['session_id'].duplicated(keep=False)]
print(all_dupes)

and to see the list of duplicated values

dupe_values = df['colname'][df['colname'].duplicated(keep=False)].unique()
print(dupe_values)


keep only the first occurrence 

df_new = df.drop_duplicates(subset='colname', keep='first')


Selecting All Instances of Duplicated Rows (including the first occurrence)

df[df.duplicated(keep=False)]


Find Duplicates Based on Specific Columns
df.duplicated(subset=['col1', 'col2'])
# Or select the duplicated rows:
df[df.duplicated(subset=['col1', 'col2'], keep=False)]


drop duplicated rows except the first occurrence 
df = df.drop_duplicates(keep='first')

If you want to reset the index afterwards:
df = df.drop_duplicates().reset_index(drop=True)v

If you want to drop duplicates only based on certain columns, e.g. 'A' and 'B':
df = df.drop_duplicates(subset=['A', 'B'], keep='first')
________________________________________________________________________
other
import pandas as pd

# Example data
early_df_unique = pd.DataFrame({
    'hh_income': ['50,000-79,999 USD', '$200,000+', '100,000-149,999 USD', '$60000-79999', 'N/A']
})

# Function to extract lower bound as float
def extract_lb(s):
    import re
    # Remove $ and commas
    s = s.replace('$', '').replace(',', '')
    # Find a sequence of digits at the start
    match = re.match(r'(\d+)', s)
    if match:
        return float(match.group(1))
    else:
        return None  # Could be NaN as well

# Apply to DataFrame
early_df_unique['hh_income_lb'] = early_df_unique['hh_income'].apply(extract_lb)

print(early_df_unique)
________________________________________________________________________
to drop rows where a specific columns is nan
df = df.dropna(subset=['col'])
________________________________________________________________________
to plot disttrib of all columns of X
import matplotlib.pyplot as plt

for col in X.columns:
    plt.figure()
    if X[col].dtype == 'object' or X[col].dtype.name == 'category':
        # Categorical bar plot
        X[col].value_counts().plot(kind='bar')
        plt.ylabel('Count')
    else:
        # Numeric histogram
        X[col].hist(bins=30)
        plt.ylabel('Frequency')
    plt.title(f'Distribution of {col}')
    plt.xlabel(col)
    plt.show()
________________________________________________________________________
It matches numbers (optionally signed, possibly decimal) inside the string.
re.findall(r"[-+]?(?:\d*\.*\d+)", """6'11"*-2.12568866m""")



def get_meter_value(x):
    if isinstance(x, str):
        m = re.findall(r"[-+]?(?:\d*\.*\d+)", x)
        if len(m) == 1:
            x = m[0]
        elif len(m) == 2:
            x = m[1]
        else:
            x = m[2]
        return x


for cols in ['Draft', 'Upwind sail area', 'Downwind sail area', 'Mainsail area', 'Maximum headroom']:
    df[cols] = df[cols].apply(get_meter_value)
________________________________________________________________________
plot heatmap correl of df
import matplotlib.pyplot as plt

plt.figure(figsize=(8, 6))  # Optional: adjust size
plt.matshow(corr, fignum=1, cmap='coolwarm')  # You can change `cmap` as desired
plt.colorbar()  # Add a color bar

plt.xticks(range(len(corr.columns)), corr.columns, rotation=90)
plt.yticks(range(len(corr.columns)), corr.columns)

plt.show()
________________________________________________________________________
boxplot to compare two groups (two df)
import matplotlib.pyplot as plt

# Your two DataFrames
# df_expensive_boat
# df_cheap_boat
boxplot_cols = ['Year', 'Hull length', 'Beam (width)', 'Draft', 'Upwind sail area']

fig, axes = plt.subplots(1, len(boxplot_cols), figsize=(15, 4), sharey=False)

for i, col in enumerate(boxplot_cols):
    # Collect data from both groups for this column
    group_data = [
        df_cheap_boat[col].dropna(),      # "Cheap boat"
        df_expensive_boat[col].dropna()   # "Expensive boat"
    ]

    # Plot boxplot for this column
    bp = axes[i].boxplot(group_data, patch_artist=True, widths=0.6)

    # Set colors (optional)
    colors = ['#1f77b4', 'green']  # cheap boat: blue, expensive boat: green
    for patch, color in zip(bp['boxes'], colors):
        patch.set_facecolor(color)

    # Set x-tick labels
    axes[i].set_xticks([1, 2])
    axes[i].set_xticklabels(['Cheap boat', 'Expensive boat'])
    axes[i].set_title(col)

plt.tight_layout()
plt.show()


OR 


fig, ax = plt.subplots()
sns.boxplot(data=df5, x="Geographic Region", y="Listing Price (USD)", ax=ax)
fig.show()
________________________________________________________________________
to create many sub boxplots to test consistency of an effect ( example : see if the price trend across region is consistent across ‘Make’)
display(boat_data['Make'].value_counts())
common_makes = boat_data['Make'].value_counts().index[:5]
fig, ax = plt.subplots(figsize=(10, 5))
sns.boxplot(data=boat_data.loc[boat_data["Make"].isin(common_makes)], x="Make", y="Listing Price (USD)", hue="Geographic Region", hue_order=["Caribbean", "Europe", "USA"], ax=ax)
ax.get_yaxis().set_major_formatter(thousands_formatter)
fig.show()
________________________________________________________________________
to plot autocorrelation by lag
import matplotlib.pyplot as plt

max_lag = 30
autocorrs = [df['value'].autocorr(lag=i) for i in range(1, max_lag+1)]

plt.figure(figsize=(10,5))
plt.bar(range(1, max_lag+1), autocorrs)
plt.xlabel('Lag')
plt.ylabel('Autocorrelation')
plt.title('Autocorrelation by Lag')
plt.show()

OR 


# Autocorrelation Plot for Seasonality Analysis

dropped_data = datasource.drop(["reference_time_start", "Marine power", "Thermal power (Coal-derived Gas, Methane)", "Thermal power (Oil Shale)", "Thermal power (Peat)", "Power generation", "Power generation (Renewables)"], 1)
dropped_data_filled = dropped_data.fillna(method='ffill').dropna()
import statsmodels.api as sm
rcParams['figure.figsize'] = 20,8.27
for feature in dropped_data.columns:
    sm.graphics.tsa.plot_acf(dropped_data_filled[feature], lags=60000)
    plt.title("Autocorrelation of {}".format(feature))
    plt.show()
________________________________________________________________________
To de seasonalize
df_daily = df_daily.set_index("Date")
decompose_data = seasonal_decompose(df_daily, model="additive", period=365)
decompose_data.plot()
plt.show()


de-seasonalized data :
decompose_data.trend + decompose_data.resid
________________________________________________________________________
dummy regressor
rom sklearn.dummy import DummyRegressor

# y_train are your training CPR values
mean_predictor = DummyRegressor(strategy='mean')
mean_predictor.fit(X_train, y_train)

# For every test observation, it predicts the training mean
y_pred = mean_predictor.predict(X_test)
________________________________________________________________________
parsing date
https://strftime.org/

Some examples:

1/17/07 has the format "%m/%d/%y"
17-1-2007 has the format "%d-%m-%Y"


------
# create a new column, date_parsed, with the parsed dates
landslides['date_parsed'] = pd.to_datetime(landslides['date'], format = "%m/%d/%y")


------

One of the biggest dangers in parsing dates is mixing up the months and days. The to_datetime() function does have very helpful error messages, but it doesn't hurt to double-check that the days of the month we've extracted make sense

# remove na's
day_of_month_landslides = day_of_month_landslides.dropna()

# plot the day of the month
sns.distplot(day_of_month_landslides, kde=False, bins=31)
________________________________________________________________________
reading files with encoding problems
# try to read in a file not in UTF-8
kickstarter_2016 = pd.read_csv("../input/kickstarter-projects/ks-projects-201612.csv")
# look at the first ten thousand bytes to guess the character encoding
with open("../input/kickstarter-projects/ks-projects-201801.csv", 'rb') as rawdata:
    result = chardet.detect(rawdata.read(10000))

# check what the character encoding might be
print(result)

So chardet is 73% confidence that the right encoding is "Windows-1252". Let's see if that's correct:


# read in the file with the encoding detected by chardet
kickstarter_2016 = pd.read_csv("../input/kickstarter-projects/ks-projects-201612.csv", encoding='Windows-1252')

# look at the first few lines
kickstarter_2016.head()
________________________________________________________________________
INCONSISTENT DATA
# get all the unique values in the 'City' column
cities = suicide_attacks['City'].unique()

# sort them alphabetically and then take a closer look
cities.sort()
cities


Just looking at this, I can see some problems due to inconsistent data entry: 'Lahore' and 'Lahore ', for example, or 'Lakki Marwat' and 'Lakki marwat'.



------
# convert to lower case
suicide_attacks['City'] = suicide_attacks['City'].str.lower()
# remove trailing white spaces
suicide_attacks['City'] = suicide_attacks['City'].str.strip()


---------

It does look like there are some remaining inconsistencies: 'd. i khan' and 'd.i khan' should probably be the same.

I'm going to use the fuzzywuzzy package to help identify which string are closest to each other.

# get the top 10 closest matches to "d.i khan"
matches = fuzzywuzzy.process.extract("d.i khan", cities, limit=10, scorer=fuzzywuzzy.fuzz.token_sort_ratio)

# take a look at them
matches

-------------

We can see that two of the items in the cities are very close to "d.i khan": "d. i khan" and "d.i khan". We can also see the "d.g khan", which is a seperate city, has a ratio of 88. Since we don't want to replace "d.g khan" with "d.i khan", let's replace all rows in our City column that have a ratio of > 90 with "d. i khan".

# function to replace rows in the provided column of the provided dataframe
# that match the provided string above the provided ratio with the provided string
def replace_matches_in_column(df, column, string_to_match, min_ratio = 90):
    # get a list of unique strings
    strings = df[column].unique()
    
    # get the top 10 closest matches to our input string
    matches = fuzzywuzzy.process.extract(string_to_match, strings, 
                                         limit=10, scorer=fuzzywuzzy.fuzz.token_sort_ratio)

    # only get matches with a ratio > 90
    close_matches = [matches[0] for matches in matches if matches[1] >= min_ratio]

    # get the rows of all the close matches in our dataframe
    rows_with_matches = df[column].isin(close_matches)

    # replace all rows with close matches with the input matches 
    df.loc[rows_with_matches, column] = string_to_match
    
    # let us know the function's done
    print("All done!")


---------------
# use the function we just wrote to replace close matches to "d.i khan" with "d.i khan"
replace_matches_in_column(df=suicide_attacks, column='City', string_to_match="d.i khan")
